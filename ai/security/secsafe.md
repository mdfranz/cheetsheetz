This is mainly about the security and safety of AI vs. [security applications of AI](secapp.md)

# Conferences and Workshops
- [AI Security Summit](https://aisecuritysummit.org/) - OASIS & Cisco - Nov '23
- [2024: Sans AI Summit](https://www.sans.org/blog/a-visual-summary-of-sans-ai-summit-2024/)

## Talks
- [Jacob Berry - No Laughing Matter: The OWASP Top 10 for LLMs in Code Examples](https://www.youtube.com/watch?v=-q2bLwC2wCI) - LASCON, Dec '24
- [Steven Rodriguez - LLM Threat Modeling for Generative AI + OWASP Top 10 LLMs](https://www.youtube.com/watch?v=vjyMp1ykOWw)
- [Attacking and Defending Gen AI: Current Threats and Vulnerabilities in LLM's](https://www.youtube.com/watch?v=ErlZvplAGCw) - Aug '24
- [Secure by Design: Strategies for LLM Adoption in Cloud-Native Environm... Patryk Bąk & Marcin Wojtas](https://www.youtube.com/watch?v=cqj86lDkJ3k) - Jul '24

# Links Pages
- [Awesome Attacks on Machine Learning Privacy](https://github.com/stratosphereips/awesome-ml-privacy-attacks)
- [OWASP Educational Links](https://owasp.org/www-project-top-10-for-large-language-model-applications/resources/)
- [Awesome LLM Red Teaming](https://github.com/user1342/Awesome-LLM-Red-Teaming)

# Vulnerabilities
- [AVID](https://avidml.org/database/) - an AI Vulnerability Taxonomy
- [OWASP Top 10 for LLM Aps](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Atlas](https://atlas.mitre.org/) - Adversarial Threat Landscape for Artificial-Intelligence Systems (think ATT&CK for AI)
- [Failure Modes in Machine Learning](https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning)

Also see *Threats and Vulnerabilities* in [MCP](./mcp/security.md)

# Frameworks
- [NIST AI Risk Management Framework](https://nist.gov/itl/ai-risk-management-framework)
- [Introducing Google’s Secure AI Framework](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)

# Vendor Best Practices 
## CSP
- [AWS: Securing GenAI](https://aws.amazon.com/ai/generative-ai/security/) and [GenAI Security Scoping Matrix](https://aws.amazon.com/blogs/security/securing-generative-ai-an-introduction-to-the-generative-ai-security-scoping-matrix/)
- [AWS: Securing Generative AI LLM Data, Applications, Workloads, Models](https://www.youtube.com/watch?v=dJPYIQb794Y) - Sep 2024
- [Azure: Security recommendations for AI workloads on Azure](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/ai/platform/security)

# Bug Bounties and CTF
- [HackAPrompt: Trick Large Language Models](https://www.aicrowd.com/challenges/hackaprompt-2023) - HackAPrompt is a prompt hacking competition aimed at enhancing AI safety and education by challenging participants to outsmart large language models (e.g. ChatGPT, GPT-3). In particlar, participants will attempt to hack through many prompt hacking defenses as possible

# Blogs
- [Mitigating Stored Prompt Injection Attacks Against LLM Applications](https://developer.nvidia.com/blog/mitigating-stored-prompt-injection-attacks-against-llm-applications/) - Aug 2023
- [A framework to securely use LLMs in companies - Part 1: Overview of Risks](https://boringappsec.substack.com/p/edition-21-a-framework-to-securely) - jul 2023
- [Guardrails for LLMOps](https://ksankar.medium.com/part-2-chatgpt-threat-vectors-guardrails-for-llmops-dbca8e0e68d4) - Jun '23
- [We need a new way to measure AI security](https://blog.trailofbits.com/2023/03/14/ai-security-safety-audit-assurance-heidy-khlaaf-odd/) - Mar '23
- [PrivacyRaven: Implementing a proof of concept for model inversion](https://blog.trailofbits.com/2021/11/09/privacyraven-implementing-a-proof-of-concept-for-model-inversion/) - Nov '21
- [PrivacyRaven Has Left the Nest](https://blog.trailofbits.com/2020/10/08/privacyraven-has-left-the-nest/) - Oct '20
# Tools
[rebuff](https://github.com/woop/rebuff) - Rebuff is designed to protect AI applications from prompt injection (PI) attacks through a multi-layered defense.

# Products & Companies
- [predictionGuargd](https://www.predictionguard.com)


# Random Tools (unsure of quality)
- https://github.com/Moshahid315/LLM-Red-Teaming-tool
- https://github.com/HuTa0kj/Warden
- https://github.com/techs-targe/llm-safety-testing-tool-v2
