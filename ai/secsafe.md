# Adversary Tooling using LLMs
- https://github.com/mrwadams/attackgen

# Security Applications
- [OpenAI Collaboration Yields 14 Recommendations for Evaluating LLMs for Cybersecurity](https://insights.sei.cmu.edu/blog/openai-collaboration-yields-14-recommendations-for-evaluating-llms-for-cybersecurity/) and [paper](https://insights.sei.cmu.edu/documents/5834/SEIOpenAICyberSecWhitepaper_FINAL.pdf) - Feb 2024

# Links Pages
- [Awesome Attacks on Machine Learning Privacy](https://github.com/stratosphereips/awesome-ml-privacy-attacks)
- [OWASP Educational Links](https://owasp.org/www-project-top-10-for-large-language-model-applications/resources/)

# Vulnerabilities
- [AVID](https://avidml.org/database/) - an AI Vulnerability Taxonomy
- [OWASP Top 10 for LLM Aps](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Atlas](https://atlas.mitre.org/) - Adversarial Threat Landscape for Artificial-Intelligence Systems (think ATT&CK for AI)
- [Failure Modes in Machine Learning](https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning)

# Frameworks
- [NIST AI Risk Management Framework](https://nist.gov/itl/ai-risk-management-framework)
- [Introducing Googleâ€™s Secure AI Framework](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)

# Bug Bounties and CTF
- [HackAPrompt: Trick Large Language Models](https://www.aicrowd.com/challenges/hackaprompt-2023) - HackAPrompt is a prompt hacking competition aimed at enhancing AI safety and education by challenging participants to outsmart large language models (e.g. ChatGPT, GPT-3). In particlar, participants will attempt to hack through many prompt hacking defenses as possible

# Blogs
- [Mitigating Stored Prompt Injection Attacks Against LLM Applications](https://developer.nvidia.com/blog/mitigating-stored-prompt-injection-attacks-against-llm-applications/) - Aug 2023
- [A framework to securely use LLMs in companies - Part 1: Overview of Risks](https://boringappsec.substack.com/p/edition-21-a-framework-to-securely) - jul 2023
- [Guardrails for LLMOps](https://ksankar.medium.com/part-2-chatgpt-threat-vectors-guardrails-for-llmops-dbca8e0e68d4) - Jun '23
- [We need a new way to measure AI security](https://blog.trailofbits.com/2023/03/14/ai-security-safety-audit-assurance-heidy-khlaaf-odd/) - Mar '23
- [PrivacyRaven: Implementing a proof of concept for model inversion](https://blog.trailofbits.com/2021/11/09/privacyraven-implementing-a-proof-of-concept-for-model-inversion/) - Nov '21
- [PrivacyRaven Has Left the Nest](https://blog.trailofbits.com/2020/10/08/privacyraven-has-left-the-nest/) - Oct '20
# Tools
[rebuff](https://github.com/woop/rebuff) - Rebuff is designed to protect AI applications from prompt injection (PI) attacks through a multi-layered defense.

# Products & Companies
- [predictionGuargd](https://www.predictionguard.com)
